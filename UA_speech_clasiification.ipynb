{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Evne6uScNwPUbwnlEubR2n98IOJw0kKa",
      "authorship_tag": "ABX9TyPyi9v2MX3e/tLyJLJKfE1R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Connect do google drive to access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sjHuct1l_xB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8217c26f-ed4e-4ded-b48b-e9eda9b6b9c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding extraction"
      ],
      "metadata": {
        "id": "DkNCJRcdsBh5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VtmEjc0rtCt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperModel, Wav2Vec2Processor, Wav2Vec2Model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Path to Audio directory\n",
        "data_dir = 'data'\n",
        "\n",
        "def load_whisper_model(model_name):\n",
        "    processor = WhisperProcessor.from_pretrained(model_name)\n",
        "    model = WhisperModel.from_pretrained(model_name)\n",
        "    return processor, model\n",
        "\n",
        "def load_wav2vec_model(model_name):\n",
        "    processor = WhisperProcessor.from_pretrained(model_name)\n",
        "    model = WhisperModel.from_pretrained(model_name)\n",
        "    return processor, model\n",
        "\n",
        "def extract_embeddings_whisper(audio_file):\n",
        "    try:\n",
        "        # Load the audio file using librosa (Resample to 16000 Hz, which Whisper expects)\n",
        "        speech_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "        # Preprocess the audio using the processor (this will create mel-spectrogram features)\n",
        "        inputs = processor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Get the mel-spectrogram features from the processor's output\n",
        "        mel_features = inputs['input_features']\n",
        "\n",
        "        # Print the shape of the mel-spectrogram to debug\n",
        "        print(f\"Mel-spectrogram shape: {mel_features.shape}\")\n",
        "\n",
        "        # Whisper expects mel-spectrograms of length 3000\n",
        "        target_length = 3000  # Whisper model's expected length\n",
        "        current_length = mel_features.shape[2]  # Access time frames dimension\n",
        "\n",
        "        if current_length < target_length:\n",
        "            # If the features are shorter than the target, pad with zeros\n",
        "            padding_length = target_length - current_length\n",
        "            mel_features = F.pad(mel_features, (0, padding_length), value=0)\n",
        "        elif current_length > target_length:\n",
        "            # If the features are longer than the target, truncate to 3000 time frames\n",
        "            mel_features = mel_features[:, :, :target_length]\n",
        "\n",
        "        # Move the inputs and model to the same device (GPU if available)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        mel_features = mel_features.to(device)\n",
        "\n",
        "        # Forward pass through the model to extract embeddings\n",
        "        # We don't need the decoder for feature extraction, so we'll pass only the encoder inputs\n",
        "        with torch.no_grad():\n",
        "            # Pass the features only through the encoder part of the model (no decoder inputs required)\n",
        "            outputs = model.encoder(input_features=mel_features, attention_mask=inputs.get('attention_mask'))\n",
        "\n",
        "        # Extract the embeddings (mean across time steps)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Averaging over time steps\n",
        "        return embeddings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_embeddings_wav2vec(audio_file):\n",
        "    try:\n",
        "        # Load the audio file using librosa\n",
        "        speech_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "        # Preprocess the audio using the processor\n",
        "        inputs = processor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Get the input values from the processor's output\n",
        "        input_values = inputs['input_values']\n",
        "\n",
        "        # Move the inputs and model to the same device (GPU if available)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        input_values = input_values.to(device)\n",
        "\n",
        "        # Forward pass through the model to extract embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_values, attention_mask=inputs.get('attention_mask'))\n",
        "\n",
        "        # Extract the embeddings (mean across time steps)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Averaging over time steps\n",
        "        return embeddings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper extraction\n",
        "model_name = \"openai/whisper-small\"\n",
        "processor, model = load_whisper_model(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Path to store embeddings\n",
        "output_dir = 'whisper_embeddings'\n",
        "speaker_folders = os.listdir(data_dir) # Create sub-folders in output dir\n",
        "for speaker in speaker_folders:\n",
        "    speaker_path = os.path.join(output_dir, speaker)\n",
        "    os.makedirs(speaker_path, exist_ok=True)\n",
        "\n",
        "\n",
        "for root, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            audio_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(audio_path, data_dir)\n",
        "            parts = relative_path.split(os.sep)\n",
        "\n",
        "            # Construct the output filename\n",
        "            if len(parts) >= 3:  # Assuming the structure is data/class_label/speaker/audio.wav\n",
        "                speaker = parts[0]\n",
        "                audio_name = os.path.splitext(file)[0]\n",
        "                output_filename = os.path.join(output_dir, speaker, f\"{audio_name}_embedding.npy\")\n",
        "\n",
        "                # Extract and save the embeddings\n",
        "                embeddings = extract_embeddings_whisper(audio_path)\n",
        "                if embeddings is not None:\n",
        "                    np.save(output_filename, embeddings)\n",
        "                    print(f\"Embeddings saved to {output_filename}\")\n",
        "\n",
        "print (\"Embedding extraction successfully completed.\")"
      ],
      "metadata": {
        "id": "QJeUwvSqsAy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wav2vec extraction\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "processor, model = load_wav2vec_model(model_name)\n",
        "\n",
        "output_dir = 'wav2vec_embeddings'\n",
        "speaker_folders = os.listdir(data_dir) # Create sub-folders in output dir\n",
        "for speaker in speaker_folders:\n",
        "    speaker_path = os.path.join(output_dir, speaker)\n",
        "    os.makedirs(speaker_path, exist_ok=True)\n",
        "\n",
        "for root, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            audio_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(audio_path, data_dir)\n",
        "            parts = relative_path.split(os.sep)\n",
        "\n",
        "            if len(parts) >= 3:  # Assuming the structure is data/class_label/speaker/audio.wav\n",
        "                speaker = parts[0]\n",
        "                audio_name = os.path.splitext(file)[0]\n",
        "                output_filename = os.path.join(output_dir, speaker, f\"{audio_name}_embedding.npy\")\n",
        "\n",
        "                embeddings = extract_embeddings_wav2vec(audio_path)\n",
        "                if embeddings is not None:\n",
        "                    np.save(output_filename, embeddings)\n",
        "                    print(f\"Embeddings saved to {output_filename}\")\n",
        "\n",
        "print (\"Embedding extraction successfully completed.\")"
      ],
      "metadata": {
        "id": "Nf5TFRE1G92O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ez7g_AF8vQFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load embedings and assign lables\n",
        "def load_embeddings_and_labels(data_dirs,label):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "        for path, label in zip(data_paths, labels):\n",
        "        for filename in os.listdir(path):\n",
        "            if filename.endswith(\".npy\"):\n",
        "                embedding = np.load(os.path.join(path, filename))\n",
        "                embeddings.append(embedding)\n",
        "                all_labels.append(label)\n",
        "    return np.array(embeddings), np.array(labels)\n",
        "\n",
        "# SVM classifier\n",
        "def perform_svm_classification(X, y, title):\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Split based on class lables\n",
        "    svm_classifier = SVC(kernel='rbf')\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy ({title}): {accuracy}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    plot_confusion_matrix(y_test, y_pred, title)\n",
        "    # plot_decision_boundary(svm_classifier, X_train, y_train, title)\n",
        "\n",
        "# Confusion matrix Plot\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(cm)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.title(f'Confusion Matrix ({title})')\n",
        "  plt.show()\n",
        "\n",
        "# PCA\n",
        "def plot_decision_boundary(classifier, X, y, title):\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    h = .02\n",
        "    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
        "    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = classifier.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.5)\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k')\n",
        "    plt.title(f'Decision Boundary ({title})')\n",
        "    plt.show()\n",
        "\n",
        "# View clasiification for each test data\n",
        "def svm_classification_results(X, y, title):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    svm_classifier = SVC(kernel='rbf')\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "    # Print predicted and actual classes for each embedding in the test set\n",
        "    for i in range(len(y_test)):\n",
        "        print(f\"Embedding {i+1}: Predicted Class - {y_pred[i]}, Actual Class - {y_test[i]}\")\n",
        "\n",
        "# Analyse class distribution across data\n",
        "def class_distribution(labels):\n",
        "    class_counts = Counter(labels)\n",
        "    total_samples = len(labels)\n",
        "    for label, count in class_counts.items():\n",
        "        percentage = (count / total_samples) * 100\n",
        "        print(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# Example usage (replace with your actual labels):\n",
        "# Assuming 'labels' is a NumPy array of your class labels\n",
        "\n",
        "# Analysing train and test data\n",
        "def svm_classification_analysis(X, y, title):\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Random samplings\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Stratified samplings\n",
        "\n",
        "    # Print the size of train and test data for each class\n",
        "    print(f\"Data sizes for {title}:\")\n",
        "    for class_label in np.unique(y):\n",
        "        train_count = np.sum(y_train == class_label)\n",
        "        test_count = np.sum(y_test == class_label)\n",
        "        print(f\"Class {class_label}: Train - {train_count}, Test - {test_count}\")"
      ],
      "metadata": {
        "id": "XCQZ1zUZvZt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper_small classification"
      ],
      "metadata": {
        "id": "xnWD6Yb6z2gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to embeddings file\n",
        "data_control='whisper_embeddings/control'\n",
        "data_verylow='whisper_embeddings/verylow'\n",
        "data_low='whisper_embeddings/low'\n",
        "data_medium='whisper_embeddings/medium'\n",
        "data_high='whisper_embeddings/high'"
      ],
      "metadata": {
        "id": "EwdFEMWoz0QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Severity classification\n",
        "data_paths = [data_verylow, data_low, data_medium, data_high]\n",
        "labels = ['verylow', 'low', 'medium', 'high']\n",
        "\n",
        "# Load embeddings and labels\n",
        "embeddings_severe, lables_severe = load_embeddings_and_labels(data_paths,labels)\n",
        "print(embeddings_severe.shape) # (data size, hidden state)\n",
        "print(lables_severe.shape)  # (label_size,)\n",
        "print(np.unique(lables_severe))\n",
        "\n",
        "# Call Svm classifier\n",
        "perform_svm_classification(embeddings_severe, lables_severe, \"Whisper_small\")"
      ],
      "metadata": {
        "id": "4KMx_HcH2gKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary classification\n",
        "data_paths = [data_control, data_verylow, data_low, data_medium, data_high]\n",
        "labels = ['Control', 'Dysarthria', 'Dysarthria', 'Dysarthria', 'Dysarthria']\n",
        "\n",
        "# Load embeddings and labels\n",
        "embeddings_binary, lables_binary = load_embeddings_and_labels(data_paths,labels)\n",
        "print(embeddings_binary.shape) # (data size, hidden state)\n",
        "print(lables_binary.shape)  # (label_size,)\n",
        "print(np.unique(lables_binary))\n",
        "\n",
        "# Call Svm classifier\n",
        "perform_svm_classification(embeddings_binary, lables_binary, \"Whisper_small\")"
      ],
      "metadata": {
        "id": "wvU7qiFh2zJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}