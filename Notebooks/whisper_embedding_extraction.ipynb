{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjbrc5Ll3fLCGxDQsqqgqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarcane-mk/whisper/blob/main/whisper_embedding_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define paths and parameters\n",
        "data_dir = \"/home/sltlab/DATA/torgo_data\"  # Replace with the actual path to your data directory\n",
        "output_dir = \"/home/sltlab/embeddings/torgo/whisper_small\"  # Replace with the desired output directory\n",
        "model_name = \"openai/whisper-small\"  # You can choose other model variants like whisper-base, whisper-large, etc.\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load WhisperProcessor and WhisperModel\n",
        "processor = WhisperProcessor.from_pretrained(model_name)\n",
        "model = WhisperModel.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to extract embeddings\n",
        "def extract_embeddings(audio_file):\n",
        "    try:\n",
        "        # Load the audio file using librosa (Resample to 16000 Hz, which Whisper expects)\n",
        "        speech_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "        # Preprocess the audio using the processor (this will create mel-spectrogram features)\n",
        "        inputs = processor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Get the mel-spectrogram features from the processor's output\n",
        "        mel_features = inputs['input_features']\n",
        "\n",
        "        # Print the shape of the mel-spectrogram to debug\n",
        "        print(f\"Mel-spectrogram shape: {mel_features.shape}\")\n",
        "\n",
        "        # Whisper expects mel-spectrograms of length 3000\n",
        "        target_length = 3000  # Whisper model's expected length\n",
        "        current_length = mel_features.shape[2]  # Access time frames dimension\n",
        "\n",
        "        if current_length < target_length:\n",
        "            # If the features are shorter than the target, pad with zeros\n",
        "            padding_length = target_length - current_length\n",
        "            mel_features = F.pad(mel_features, (0, padding_length), value=0)\n",
        "        elif current_length > target_length:\n",
        "            # If the features are longer than the target, truncate to 3000 time frames\n",
        "            mel_features = mel_features[:, :, :target_length]\n",
        "\n",
        "        # Move the inputs and model to the same device (GPU if available)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        mel_features = mel_features.to(device)\n",
        "\n",
        "        # Forward pass through the model to extract embeddings\n",
        "        # We don't need the decoder for feature extraction, so we'll pass only the encoder inputs\n",
        "        with torch.no_grad():\n",
        "            # Pass the features only through the encoder part of the model (no decoder inputs required)\n",
        "            outputs = model.encoder(input_features=mel_features, attention_mask=inputs.get('attention_mask'))\n",
        "\n",
        "        # Extract the embeddings (mean across time steps)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Averaging over time steps\n",
        "        return embeddings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Iterate through all audio files in the data directory\n",
        "for root, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            audio_path = os.path.join(root, file)\n",
        "            audio_name = os.path.splitext(file)[0]  # Remove the file extension\n",
        "\n",
        "            # Construct the output filename\n",
        "            output_filename = os.path.join(output_dir, f\"{audio_name}_embedding.npy\")\n",
        "\n",
        "            # Extract embeddings and save to file\n",
        "            embeddings = extract_embeddings(audio_path)\n",
        "            if embeddings is not None:\n",
        "                np.save(output_filename, embeddings)\n",
        "                print(f\"Embeddings for {audio_name} saved to {output_filename}\")\n"
      ],
      "metadata": {
        "id": "Zp33l-KZ9jmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
